



Tiny test
---------

    make setup_database variable=TEMP dir=hdfs://icme-hadoop1.localdomain/user/jatempl/random_media/run? outdir=hdfs://icme-hadoop1.localdomain/user/dgleich/rm-sisc/dbtest name=dbtest

   mv dbtest dbtest.dgleich

Add numParams?=64
    numExodusfiles?=16
    timesteps?=tstepstest.txt
    mr_seq2mseq2_hadoop.py -> mr_seq2mseq5_hadoop.py

     (5 is a special version to parse the parameter numbers correctly for this
one)

    make -f dbtest.dgleich preprocess


Use this timesteps file

cat tstepstest.txt 
1250.0
1650.0
2000.0

Fix up the input list

    bash make_db_test_input.sh

    hadoop fs -rm
hdfs://icme-hadoop1.localdomain/user/dgleich/rm-sisc/dbtest/input.txt
    hadoop fs -put inputtest.txt
hdfs://icme-hadoop1.localdomain/user/dgleich/rm-sisc/dbtest/input.txt 

    make -f dbtest.dgleich convert
    make -f dbtest.dgleich seq2mseq2

Store the matrix for processing myself

    cd model
    python dump_mseq2.py -hadoop /usr/lib/hadoop -mat rm-sisc/dbtest/data.mseq2
    mkdir ../../../experiments/dbtest
    hadoop fs -text /user/dgleich/rm-sisc/dbtest/data.mtxt/part-00000.deflate > ../../../experiments/dbtest/data.mtxt
    cd ..
    cp tstepstest.txt ../../experiments/dbtest/tsteps.txt
    cd 

On my desktop

    # work in the ssh shared directory
    cd ~/remote/icme-hadoop1/simform-sisc/experiments/dbtest
    convert_dbtest
    
Then we can use

    split_test2.m to generate true errors    
    
That will save dbtest.mat into ~/scratch we can use it to verify the SVD

    make -f dbtest.dgleich model # compute the full SVD
    make -f dbtest.dgleich train # compute an SVD on a training subset
    
Let's do some prediction! The code paul wrote does:
python compute_interp_weights.py Vtfile Sigmafile designpoints interppoints > dbtest-weights.txt

dbtest parameter set    
1 5 9 13 17 21 25 29 33 37 41 45 49 53 57 61
trainset design
1 17 33 49
testset design
9 25 41     

Let's make sure it's right

    cat dbtest-trainset-design.txt    
    cat dbtest-testset-design.txt

Now let's use Paul's script to compute things!

    python compute_interp_weights.py model/svd-test-train/Vt.txt.out \
      model/svd-test-train/Sigma.txt.out \
      dbtest-trainset-design.txt  dbtest-testset-design.txt R 4 > dbtest-weights.txt
      
      

python mr_predict2_hadoop.py \
  hdfs://icme-hadoop1.localdomain/user/dgleich/rm-sisc/dbtest/train_3/p* \
  -r hadoop --no-output -o rm-sisc/dbtest/predict \
  --weights=dbtest-weights.txt --file dbtest-weights.txt \
  --subset=9,25,41



python mr_error_hadoop.py \
hdfs://icme-hadoop1.localdomain/user/dgleich/rm-sisc/dbtest/data.seq/random_media/random_media*.seq \
hdfs://icme-hadoop1.localdomain/user/dgleich/rm-sisc/dbtest/predict/p* \
-r hadoop --no-output -o rm-sisc/dbtest/error --numParas 64  --variable TEMP \
--subset=9,25,41
  
Summary of notes while working on this. The initial SVD was wrong due to the
'uotp' bug in the svd code. So I fixed that. Also 

I validated this by making sure the error was the same against the ones 
computed in Matlab.

    # make sure it's a REAL tab after the 9 (in linux, <ctrl-v> <TAB>)
    dumbo cat rm-sisc/dbtest/error/part-* -hadoop /usr/lib/hadoop | \
        grep "9 " | cut -f 4 

One of the key problems was that the first elements of the predicted
scores was not 1000, which it's fixed to in all of the simulations, so this
indicates a bug.

    dumbo cat rm-sisc/dbtest/predict/part-00007 -hadoop /usr/lib/hadoop | less
    
    
Generate a small database for initial testing testing
-----------

    make setup_database variable=TEMP dir=hdfs://icme-hadoop1.localdomain/user/jatempl/random_media/run? outdir=hdfs://icme-hadoop1.localdomain/user/dgleich/rm-sisc/dbsmall name=dbsmall

Change it's input file

    make -f dbsmall preprocess

    bash make_db_small_input.sh

    hadoop fs -rm hdfs://icme-hadoop1.localdomain/user/dgleich/rm-sisc/dbsmall/input.txt
    hadoop fs -put input.txt hdfs://icme-hadoop1.localdomain/user/dgleich/rm-sisc/dbsmall/input.txt

Convert it to seq files

    make -f dbsmall convert
    
Check the timesteps    
    
    python mr_seq_timestep_check.py \
    hdfs://icme-hadoop1.localdomain/user/dgleich/rm-sisc/dbsmall/data.seq/random_media/random_media*.seq \
    -r hadoop --no-output --variable TEMP -o hdfs://icme-hadoop1.localdomain/user/dgleich/rm-sisc/dbsmall/timesteps.seq
    
    dumbo cat hdfs://icme-hadoop1.localdomain/user/dgleich/rm-sisc/dbsmall/timesteps.seq

The output of this file is:

timestep index     time step     simulation number


Full database
-------------



Create the time-steps file to normalize all the times:

    cat tsteps.txt
    1250.0
    1350.0
    1450.0
    1550.0
    1650.0
    1750.0
    1850.0
    1950.0
    2000.0



Dump out the farface

time python mr_exodus2farface.py \
     hdfs://icme-hadoop1.localdomain/user/dgleich/rm-sisc/dbfull/input.txt \
        -r hadoop --python-archive simform-deploy.tar.gz \
        --variables TEMP -o
hdfs://icme-hadoop1.localdomain/user/dgleich/rm-sisc/dbfull/farface.txt
--no-output  \
        --jobconf mapred.line.input.format.linespermap=10
        
Took 
 2hrs, 24mins, 21sec        

Convert

Add jobconf linespermap=10 to dbfull

make -f dbfull convert timestepfile=tsteps.txt 

Time = 
real    883m10.961s


real    1968m22.642s
user    2m3.234s
sys     1m17.896s

(2x replication)
real    1211m56.694s

Convert to matrix sequence files Done!



We are now space constrained ...

./bin/hadoop dfs -setrep -R -w 2 /user/dgleich/rm-sisc/dbfull/data.mseq2
./bin/hadoop dfs -setrep -R -w 1 /user/dgleich/rm-sisc/dbfull/data.seq

Oops, that was a REALLY bad idea as one of the disks failed.

We are going to patch what's left in data.seq

hadoop fsck /user/dgleich/rm-sisc/dbfull > corrupted.txt
hadoop fsck /user/dgleich/rm-sisc/dbfull -delete | tee fsck-output.txt

grep "data.seq/random_media" corrupted.txt  | \
   perl -nle 'print "hdfs://icme-hadoop1.localdomain/user/jatempl/random_media/run$1/random_media.e" if (/random_media\/random_media([\d]*)/)' > patch-2.txt


Model failed after full1.py completed. We were able to reset

Full 1 took 4hrs, 38mins, 25sec with 40 cores.

dumbo start full2.py -hadoop /usr/lib/hadoop -mat
  hdfs://icme-hadoop1.localdomain/user/dgleich/rm-sisc/dbfull/model_1/R_* \
  -output hdfs://icme-hadoop1.localdomain/user/dgleich/rm-sisc/dbfull/model_2 \
  -svd 3 -nummaptasks 100 -libjar feathers.jar -jobconf \
  mapred.child.java.opts="-Xmx12G" -jobconf mapred.map.max.attempts=10 \
  -jobconf mapred.task.timeout=1200000 -memlimit 20g

hadoop fs -copyToLocal
hdfs://icme-hadoop1.localdomain/user/dgleich/rm-sisc/dbfull/model_2/Q2/part-00000
full-svd/Q2.txt
python
/home/dgleich/simform-sisc/simform/src/model/hyy-python-hadoop/examples/SequenceFileReader.py
full-svd/Q2.txt > full-svd/Q2.txt.out

hadoop fs -copyToLocal
hdfs://icme-hadoop1.localdomain/user/dgleich/rm-sisc/dbfull/model_2/U/part-00000
full-svd/U.txt
python
/home/dgleich/simform-sisc/simform/src/model/hyy-python-hadoop/examples/SequenceFileReader.py
full-svd/U.txt > full-svd/U.txt.out

hadoop fs -copyToLocal
hdfs://icme-hadoop1.localdomain/user/dgleich/rm-sisc/dbfull/model_2/Sigma/part-00000
full-svd/Sigma.txt
python
/home/dgleich/simform-sisc/simform/src/model/hyy-python-hadoop/examples/SequenceFileReader.py
full-svd/Sigma.txt > full-svd/Sigma.txt.out

hadoop fs -copyToLocal
hdfs://icme-hadoop1.localdomain/user/dgleich/rm-sisc/dbfull/model_2/Vt/part-00000
full-svd/Vt.txt
python
/home/dgleich/simform-sisc/simform/src/model/hyy-python-hadoop/examples/SequenceFileReader.py
full-svd/Vt.txt > full-svd/Vt.txt.out

Took 1 hr 10 mins with 20 cores.

dumbo start full3.py -hadoop /usr/lib/hadoop -mat
hdfs://icme-hadoop1.localdomain/user/dgleich/rm-sisc/dbfull/model_1/Q_*
-output hdfs://icme-hadoop1.localdomain/user/dgleich/rm-sisc/dbfull/model_3
-ncols 64 -q2path full-svd/Q2.txt.out -nummaptasks 100 -libjar feathers.jar
-jobconf mapred.child.java.opts="-Xmx2G" -jobconf dfs.replication=1 -jobconf
mapred.map.max.attempts=10 -jobconf mapred.task.timeout=1200000


cat Sigma.txt.out | perl -nle 'print $1 if (/\[(.*)\]/)' | sed s/\,//g > Sigma.txt
cat Vt.txt.out | perl -nle 'print $1 if (/\[(.*)\]/)' | sed s/\,//g > Vt.txt

Create the training model

    make -f dbtest train

Create the testing interpolation points

    bash make_sisc_train_weights.txt      


python mr_predict2_hadoop.py \
  hdfs://icme-hadoop1.localdomain/user/dgleich/rm-sisc/dbfull/train_3/p* \
  -r hadoop --no-output -o rm-sisc/dbfull/predict \
  --weights=sisc-train-weights-16.txt --file sisc-train-weights-16.txt \
  --subset=3,7,11,15,19,23,27,31,35,39,43,47,51,55,59 

python mr_error_hadoop.py \
hdfs://icme-hadoop1.localdomain/user/dgleich/rm-sisc/dbfull/data.seq/random_media/random_media*.seq \
hdfs://icme-hadoop1.localdomain/user/dgleich/rm-sisc/dbfull/predict/p* \
-r hadoop --no-output -o rm-sisc/dbfull/error --numParas 64  --variable TEMP \
--jobconf mapred.reduce.tasks=10000 \
--jobconf mapred.reduce.slowstart.completed.maps=1 \
--subset=3,7,11,15,19,23,27,31,35,39,43,47,51,55,59 


---

Okay, that took WAY too long. (About 24 hours to compute the errors...)

I rewrote the code to do it all in-situ :-) see full3cv.py

Let's make a parameter file

    rm sisc-params.txt
    touch sisc-params.txt
    for i in $(seq 1 64); do echo $i >> sisc-params.txt; done
    
Now let's call the training and cross-validation routine

    cd model
    python run_full_tsqr_cv.py --parameter-file=../sisc-params.txt \
        --train-subset=0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60 \
        --test-subset=2,6,10,14,18,22,26,30,34,38,42,46,50,54,58  \
        --replication=1 --javamem=7g \
        --input=rm-sisc/dbfull/data.mseq2/ \
        --hadoop=/usr/lib/hadoop \
        --local_output=train-svd-cv \
        --output=rm-sisc/dbfull/traincv 
        
Arg, when I first ran that, I had the wrong test set.  Here is what
I did to fix it.

    # the incorrect command that computed the wrong errors
    # dumbo start full3cv.py -hadoop /usr/lib/hadoop -mat rm-sisc/dbfull/traincv_1/Q_* \
    #   -output rm-sisc/dbfull/traincv_3 -q2path train-svd-cv/Q2.txt.out \
    #   -libjar feathers.jar -subset 0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60 \
    #   -test_subset 3,7,11,15,19,23,27,31,35,39,43,47,51,55,59 -ppath ../sisc-params.txt \
    #   -upath train-svd-cv/U.txt.out -sigmapath train-svd-cv/Sigma.txt.out -vtpath train-svd-cv/Vt.txt.out \
    #   -jobconf mapred.child.java.opts="-Xmx7g" -jobconf dfs.replication=1 \
    # -jobconf mapred.map.max.attempts=10 -jobconf mapred.task.timeout=1200000
    # now the fixed command
    dumbo start full3cv.py -hadoop /usr/lib/hadoop -mat rm-sisc/dbfull/traincv_1/Q_* \
        -output rm-sisc/dbfull/traincv_3 -q2path train-svd-cv/Q2.txt.out \
        -libjar feathers.jar -subset 0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60 \
        -test_subset 2,6,10,14,18,22,26,30,34,38,42,46,50,54,58 -ppath ../sisc-params.txt \
        -upath train-svd-cv/U.txt.out -sigmapath train-svd-cv/Sigma.txt.out -vtpath train-svd-cv/Vt.txt.out \
        -jobconf mapred.child.java.opts="-Xmx7g" -jobconf dfs.replication=1 \
        -jobconf mapred.map.max.attempts=10 -jobconf mapred.task.timeout=1200000

    # this proved to be much faster. dumbo cat part-* does a dumptb for each individual file! ACK!
    
    dumbo start catall.py -input rm-sisc/dbfull/traincv_3 -output rm-sisc/dbfull/traincv_err -jobconf dfs.replication=1 -overwrite yes
    dumbo cat rm-sisc/dbfull/traincv_err/part-* -hadoop /usr/lib/hadoop | python parse_errors.py > train-svd-cv/errs.txt        



And let's get the results for the full problem

    python run_full_tsqr_cv.py --parameter-file=../sisc-params.txt \
        --train-subset=0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62 \
        --test-subset=1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61  \
        --replication=1 --javamem=7g \
        --input=rm-sisc/dbfull/data.mseq2/ \
        --hadoop=/usr/lib/hadoop \
        --local_output=test-svd-cv \
        --output=rm-sisc/dbfull/testcv 

    time dumbo start catall.py -input rm-sisc/dbfull/testcv_3 -output rm-sisc/dbfull/testcv_err -jobconf dfs.replication=1 -overwrite yes
    
    real    41m18.057s
    user    0m13.233s
    sys     0m3.246s

    date
    dumbo cat rm-sisc/dbfull/testcv_err/part-* -hadoop /usr/lib/hadoop | python parse_errors.py > test-svd-cv/errs2.txt        
    date
    
    Fri Jun  7 09:43:25 PDT 2013
    Jun  7 10:44
    
Look at one solution with high error

    Predict, get error and variance
    
    

# This is a makefile script to run scripts automatically
#
# Example usage:
#     make -f dbsmall convert # convert exodus files to sequence files
#     make -f dbsmall model # compute the SVD
#     make -f dbsmall predict design=<design sites> points=<interpolation sites>
#     make -f dbsmall check # make sure the database is "correct"
#     make -f dbsmall var # compute the variance of the dataset
#
# Yangyang Hou  hou13@purdue.edu
# Copyright (c) 2012

#
# HEAVILY MODIFIED FOR THE SISC PAPER
# 


dir?=hdfs://icme-hadoop1.localdomain/user/jatempl/random_media/run?/
variable?=TEMP
outdir?=hdfs://icme-hadoop1.localdomain/user/dgleich/rm-sisc/dbsmall/

SVD?=False

timesteps?=10
tmpdir?=./
timestepfile?=tsteps.txt
numExodusfiles?=64
numParas?=64

inputfile=$(outdir)input.txt
exodus2seq_output?=$(outdir)data.seq/

var_input?=$(exodus2seq_output)*/*part*.seq
var_output?=$(outdir)data.var/

exodusvar_input?=$(var_output)p*
exodusvar_output?=$(outdir)exodus.var/

mseq_input=$(exodus2seq_output)*/*part*.seq
mseq_output?=$(outdir)data.mseq/
mseq2_output?=$(outdir)data.mseq2/

train_input?=$(mseq2_output)
train_subset?=1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61
train_output?=$(outdir)train




model_input?=$(mseq_output)
model_output?=$(outdir)model

predict_input=$(exodus2seq_output)*/*part*.seq
predict_output?=$(outdir)predict/noSVD/
design?=
points?=


predict_SVD_input?=$(model_output)_4/
predict_SVD_output?=$(outdir)predict/SVD/
weights?=

seq2exodus_input?=$(predict_output)part*
seq2exodus_output?=$(outdir)interpexodus_noSVD/

seq2exodus_SVD_input?=$(predict_SVD_output)part*
seq2exodus_SVD_output?=$(outdir)interpexodus_SVD/
output_name?=

train: 
	@echo 'computing matrix for training'
	python run_full_tsqr_subset.py --input=$(mseq2_output) \
	--subset=$(train_subset) --svd=2 --schedule=1200,100,1200 \
	--hadoop=/usr/lib/hadoop --local_output=tsqr-train \
	--output=$(train_output)
    
train_weights.txt: 
	python 
    
# predict from tranining data    
train_predict: train_weights.txt
    @echo 'computing predictions


# create weights.txt file for predicting with SVD

predict:mr_predict_hadoop.py mr_predictwithSVD_hadoop.py 
ifeq ($(SVD),False)
	@echo '========================================';\
	echo 'Predicting directly using existing exodus files database...';\
	test -e $(design) || echo 'The design sites file does not exist!'; \
	test -e $(points) || echo 'The interpolation sites file does not exist!'; \
	test -e $(design) && test -e $(points) && \
	hadoop fs -test -e $(predict_output) && \
	python check_time.py $(exodus2seq_output) $(predict_output) && \
	hadoop fs -rmr $(predict_output);\
	test -e $(design) && test -e $(points) && \
	(hadoop fs -test -e $(predict_output) || \
	time python mr_predict_hadoop.py $(predict_input) -r hadoop --no-output \
	-o $(predict_output) --variable $(variable) \
	--design=$(design) --points=$(points) --file $(design) --file $(points)); \
	echo 'Prediction Done!'; \
	echo '========================================' 
else
ifeq ($(SVD),True)
	@echo '========================================';\
	echo 'Predicting with SVD model...';\
	test -e $(weights) || echo 'The weights file does not exist!'; \
	test -e $(weights) && \
	hadoop fs -test -e $(predict_SVD_output) && \
	python check_time.py $(predict_SVD_input) $(predict_SVD_output) && \
	hadoop fs -rmr $(predict_SVD_output); \
	test -e $(weights) && \
	(hadoop fs -test -e $(predict_SVD_output) || \
	time python mr_predictwithSVD_hadoop.py $(predict_SVD_input) -r hadoop --no-output \
	-o $(predict_SVD_output)  \
	--weights=$(weights)  --file $(weights) ); \
	echo 'Prediction Done!'; \
	echo '========================================' 
endif
endif

seq2exodus: mr_outputexodus_hadoop.py mr_outputexoduswithSVD_hadoop.py simform-deploy.tar.gz
ifeq ($(SVD),False)
	@echo '========================================'; \
	echo 'Converting interpolated sequence files to interpolated exodus files...'; \
	hadoop fs -test -e $(seq2exodus_output) && \
	python check_time.py $(predict_output) $(seq2exodus_output) && \
	hadoop fs -rmr $(seq2exodus_output);\
	hadoop fs -test -e $(seq2exodus_output) || \
	time python mr_outputexodus_hadoop.py $(seq2exodus_input) --outputname $(output_name) \
	--variable $(variable)  --outdir $(seq2exodus_output) --indir $(dir) -r hadoop \
	--python-archive simform-deploy.tar.gz --no-output; \
	echo 'Convert to new interpolated exodus files Done!';\
	echo '========================================'
else
ifeq ($(SVD),True)
	@echo '========================================'; \
	echo 'Converting interpolated sequence files to interpolated exodus files with SVD...'; \
	hadoop fs -test -e $(seq2exodus_SVD_output) && \
	python check_time.py $(predict_SVD_output) $(seq2exodus_SVD_output) && \
	hadoop fs -rmr $(seq2exodus_SVD_output);\
	hadoop fs -test -e $(seq2exodus_SVD_output) || \
	time python mr_outputexoduswithSVD_hadoop.py $(seq2exodus_SVD_input) --outputname $(output_name) \
	--variable $(variable)  --outdir $(seq2exodus_SVD_output) --indir $(dir) -r hadoop \
	--python-archive simform-deploy.tar.gz --no-output; \
	echo 'Convert to new interpolated exodus files Done!'; \
	echo '========================================'
endif
endif

check:

.PHONY: clean 
clean:
	@echo '========================================';\
	echo 'Removing temporary files in local and on HDFS...';\
	test -e $(tmpdir)tmp && rm -r $(tmpdir)tmp;\
	hadoop fs -rm $(inputfile);\
	echo 'Clean Done!'; \
	echo '========================================'
